{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2acaa01-ef42-4537-8868-77530dc34e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "DATA_FLOW = 'local'\n",
    "DATA_SAMPLE = .1\n",
    "SAMPLE_OUTLIERS = 0.75\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    text = text.replace('@', '')\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Read news data\n",
    "if DATA_FLOW == 'local':\n",
    "    news_tweets = pd.read_csv(\"./../data/processed/news_tweets_clean.csv\")\n",
    "elif DATA_FLOW == 'gcp':\n",
    "    from google.cloud import storage\n",
    "    from io import BytesIO\n",
    "    client = storage.Client()\n",
    "    bucket_name = \"covid-news-response\"\n",
    "    file_name = \"news_tweets_clean.csv\"\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.get_blob(file_name)\n",
    "    content = blob.download_as_string()\n",
    "    news_tweets = pd.read_csv(BytesIO(content))\n",
    "\n",
    "# Country filtering\n",
    "tweet_topics = pd.DataFrame([])\n",
    "#for country in news_tweets.country.unique():\n",
    "for country in ['AU', 'NZ', 'MY']:\n",
    "    df = news_tweets[news_tweets.country==country]\n",
    "\n",
    "    # Clear news data\n",
    "    df = df.drop_duplicates()\n",
    "    df.created_at = pd.to_datetime(df.created_at)\n",
    "    df['clean_text'] = df.text.apply(lambda x: preprocess(x))\n",
    "    df['moy'] = pd.to_datetime(\n",
    "        df.created_at.dt.year.astype(str) + '-' + df.created_at.dt.month.astype(str) + '-1')\n",
    "    df_sample = df.sample(frac=DATA_SAMPLE, random_state=42)\n",
    "    docs = df_sample.clean_text.values\n",
    "    timestamps = df_sample.moy.to_list()\n",
    "\n",
    "    # Constants\n",
    "    QUANTITY = len(docs)\n",
    "    MIN_CLUSTER_SIZE = int(np.ceil(QUANTITY * 0.0005))\n",
    "    N_NEIGHBORS = int(np.ceil(MIN_CLUSTER_SIZE * 0.75))\n",
    "    MIN_SAMPLES = int(np.ceil(MIN_CLUSTER_SIZE * SAMPLE_OUTLIERS))\n",
    "    MIN_VECTORIZE = int(np.ceil(QUANTITY * 2e-05))\n",
    "\n",
    "    # Topic model\n",
    "    vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=\"english\", min_df=MIN_VECTORIZE)\n",
    "    umap_model = UMAP(n_neighbors=N_NEIGHBORS, metric='cosine', low_memory=True)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=MIN_CLUSTER_SIZE, min_samples=MIN_SAMPLES, metric='euclidean')\n",
    "    topic_model = BERTopic(\n",
    "        language=\"english\", nr_topics=\"auto\", calculate_probabilities=False, verbose=True, low_memory=True,\n",
    "        vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    df_sample['topic'] = topics\n",
    "    tweet_topics = pd.concat([tweet_topics, df_sample[['tweet_id', 'topic']]])\n",
    "    \n",
    "    # Saving\n",
    "    topic_model.save(fr\"./models/topic_model_{country}.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed5353-9e71-44e0-80a1-78e6b3d7cdb3",
   "metadata": {},
   "source": [
    "### Check topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3434d-8b07-445c-9c20-d4d79c2fca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "print(\"The number of documents is:\", QUANTITY)\n",
    "print(\"The number of identified topics is:\", len(freq))\n",
    "print(\"The percentage of outlier documents is:\", np.round(freq[freq.Topic==-1]['Count'].values[0]/QUANTITY,2))\n",
    "freq.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdeb805-4ca6-426d-a9f1-5acbb8cbb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eba100-2155-4996-b5fc-36aa392c4ada",
   "metadata": {},
   "source": [
    "### Visualize topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a31db-cab2-4023-b4a8-6493471b2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0afa19-9be8-4ba7-81e7-2299f73ca658",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f67574-6225-46fe-992a-b23b2d7306f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57a752-45b4-463b-aab2-ceabe3cc958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(top_n_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0b5d4-bb71-48d5-a586-b76c954adcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_term_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ba19a-915e-405d-b8c1-59c173cecfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, topics=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c87e1-4386-4a7e-b4fe-f92204af55e2",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea57fd-b1c5-4b6c-8776-a7ff7c8cd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if DATA_FLOW == 'local':\n",
    "    topic_model.save(\"./../models/topic_model_bertopic.sav\")\n",
    "    df_sample[['tweet_id', 'moy', 'topic']].to_csv('./../data/processed/df_topics.csv')\n",
    "elif DATA_FLOW == 'gcp':\n",
    "    topic_model.save(\"topic_model_bertopic.sav\")\n",
    "    df_sample[['tweet_id', 'moy', 'topic']].to_csv('df_topics.csv')\n",
    "    !gsutil df_topics.csv gs://covid-news-response/\n",
    "    !gsutil topic_model_bertopic.sav gs://covid-news-response/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
