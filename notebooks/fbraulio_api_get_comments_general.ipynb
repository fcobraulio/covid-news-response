{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aa331bd20288>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeocoders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNominatim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrate_limiter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRateLimiter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopy'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "import time\n",
    "import random\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "def auth():\n",
    "    #return os.getenv('BEARER_TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def append_tweet_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet and Conversation ID\n",
    "        tweet_id = tweet['id']\n",
    "        conversation_id = tweet['conversation_id']\n",
    "        in_reply_to_user_id = tweet['in_reply_to_user_id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [tweet_id, conversation_id, author_id, in_reply_to_user_id, created_at, geo, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)\n",
    "    \n",
    "def append_user_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for user in json_response['includes']['users']:\n",
    "\n",
    "        # 1. Author ID, Username and DisplayName\n",
    "        author_id = user['id']\n",
    "        username = user['username']\n",
    "        display_name = user['name']\n",
    "\n",
    "        # 2. Description\n",
    "        description = user['description']\n",
    "        \n",
    "        # 3. Verified\n",
    "        verified = user['verified']\n",
    "        \n",
    "        # 4. Time user created\n",
    "        created_at = dateutil.parser.parse(user['created_at'])\n",
    "\n",
    "        # 5. User metrics\n",
    "        followers_count = user['public_metrics']['followers_count']\n",
    "        following_count = user['public_metrics']['following_count']\n",
    "        tweet_count = user['public_metrics']['tweet_count']\n",
    "        listed_count = user['public_metrics']['listed_count']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, username, display_name, description, verified, created_at, followers_count, following_count, tweet_count, listed_count]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Users added from this response: \", counter) \n",
    "    \n",
    "def append_place_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for place in json_response['includes']['places']:\n",
    "\n",
    "        # Features\n",
    "        geo = place['id']\n",
    "        full_name = place['full_name']\n",
    "        place_type = place['place_type']\n",
    "        name = place['name']\n",
    "        country_code = place['country_code']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [geo, full_name, place_type, name, country_code]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Places added from this response: \", counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "priority_news = ['CNN','MSNBC','NBCNews','FoxNews','BBCNews','itvnews','SkyNews','CTVNews','CBCNews','globalnews','inquirerdotnet','ABSCBNNews','gmanews','ndtv','timesofindia','TimesNow','republic','7NEWS','9NewsAUS','abcnews','News24','eNCA','SABCNews','rtenews','Independent_ie','thejournal_ie','MobilePunch','vanguardngrnews','PulseNigeria247','citizentvkenya','ntvkenya','NationAfrica','ntvuganda','nbstv','DailyMonitor','malaysiakini','staronline','NewshubNZ','nzherald']\n",
    "\n",
    "#\n",
    "df = pd.read_csv('./../data/processed/news_tweets.csv')\n",
    "news_accounts = pd.read_csv('./../data/processed/covid_users.csv')\n",
    "skynews_conversations = pd.read_csv('./../data/processed/skynews/tweets_replies_skynews.csv')\n",
    "skynews_conversations = list(skynews_conversations.conversation_id.unique())\n",
    "tmp = df.author_id.value_counts().rename('amount').rename_axis('author_id').reset_index()\n",
    "news_accounts = news_accounts[news_accounts.author_id.isin(tmp.author_id)].drop_duplicates('author_id').merge(tmp).sort_values('amount', ascending=False)\n",
    "\n",
    "# Get selected conversations\n",
    "conversations = list(df[(df.author_id.isin(news_accounts[news_accounts.username.isin(priority_news)].author_id.values)) & (df.reply_count>0)].conversation_id)\n",
    "conversations = np.setdiff1d(conversations,skynews_conversations)\n",
    "random.shuffle(conversations)\n",
    "batches = list(split(conversations, int(len(conversations)/24)))\n",
    "\n",
    "# Create tweet file and write header\n",
    "csvFile = open(\"./../data/processed/user_responses/tweets_replies.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['tweet_id', 'conversation_id', 'author_id', 'in_reply_to_user_id', 'created_at', 'geo', 'lang', 'like_count', 'quote_count', 'reply_count', 'retweet_count', 'source', 'text'])\n",
    "csvFile.close()\n",
    "\n",
    "# Create user file and write header\n",
    "csvFile = open(\"./../data/processed/user_responses/users_replies.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['author_id', 'username', 'display_name', 'description', 'verified', 'created_at', 'followers_count', 'following_count', 'tweet_count', 'listed_count'])\n",
    "csvFile.close()\n",
    "\n",
    "# Create place file and write header\n",
    "csvFile = open(\"./../data/processed/user_responses/places_replies.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['geo', 'full_name', 'place_type', 'name', 'country_code'])\n",
    "csvFile.close()\n",
    "\n",
    "# Inputs for tweets\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "start_date = '2020-01-01T00:00:00.000Z'\n",
    "end_date = '2021-11-14T23:59:59.000Z'\n",
    "max_results = 500\n",
    "\n",
    "# Loop Inputs\n",
    "total_tweets = 0\n",
    "count = 0 # Counting tweets per time period\n",
    "max_count = 4556874 # Max tweets per time period\n",
    "flag = True\n",
    "next_token = None\n",
    "n_requests = 0\n",
    "n_batches = 0\n",
    "valid = False\n",
    "errCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if flag is true\n",
    "start_time = time.time()\n",
    "\n",
    "for batch in batches:\n",
    "    \n",
    "    search = \"conversation_id:\" + \" OR conversation_id:\".join( [str(s) for s in batch]) + \" lang:en is:reply -is:retweet\"\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"Batch #: \", n_batches)\n",
    "    total_loop_tweets = 0\n",
    "    flag = True\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"Request #: \", n_requests+1, \" | Time cap: \", int(time.time() - start_time))\n",
    "        print(\"Token: \", next_token)\n",
    "        while not valid:\n",
    "            try:\n",
    "                url = create_url(search, start_date,end_date, max_results)\n",
    "                json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "                result_count = json_response['meta']['result_count']\n",
    "                n_requests += 1\n",
    "                valid = True; errCount = 0\n",
    "            except:\n",
    "                errCount += 1\n",
    "                time.sleep(2^errCount)\n",
    "                print(\"-------------------------------------- Request error #\", errCount)\n",
    "        valid = False\n",
    "\n",
    "        # Save the token to use for next call\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                append_tweet_to_csv(json_response, \"./../data/processed/user_responses/tweets_replies.csv\")\n",
    "                append_user_to_csv(json_response, \"./../data/processed/user_responses/users_replies.csv\")\n",
    "                if 'places' in json_response['includes'].keys():\n",
    "                    append_place_to_csv(json_response, \"./../data/processed/user_responses/places_replies.csv\")\n",
    "                count += result_count\n",
    "                total_loop_tweets += result_count\n",
    "                print(\"Cumulative # of Tweets in this batch: \", total_loop_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(1)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                append_tweet_to_csv(json_response, \"./../data/processed/user_responses/tweets_replies.csv\")\n",
    "                append_user_to_csv(json_response, \"./../data/processed/user_responses/users_replies.csv\")\n",
    "                if 'places' in json_response['includes'].keys():\n",
    "                    append_place_to_csv(json_response, \"./../data/processed/user_responses/places_replies.csv\")\n",
    "                count += result_count\n",
    "                total_loop_tweets += result_count\n",
    "                print(\"Cumulative # of Tweets in this batch: \", total_loop_tweets)\n",
    "                time.sleep(1)\n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(1)\n",
    "        # If reachs requests cap, stop it\n",
    "        t = time.time()-start_time\n",
    "        if n_requests==300:\n",
    "            if t<900:\n",
    "                time.sleep(900 - t)\n",
    "            start_time=time.time()\n",
    "            n_requests = 0\n",
    "    n_batches += 1\n",
    "    total_tweets += total_loop_tweets\n",
    "    print(\"Total number of batch results: \", total_loop_tweets)\n",
    "    print(\"Total # of Tweets added: \", total_tweets)\n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
