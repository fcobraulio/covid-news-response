{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "# from geopy.geocoders import Nominatim\n",
    "# from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "def auth():\n",
    "    #return os.getenv('BEARER_TOKEN')\n",
    "    return os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def append_tweet_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet and Conversation ID\n",
    "        tweet_id = tweet['id']\n",
    "        conversation_id = tweet['conversation_id']\n",
    "        in_reply_to_user_id = tweet['in_reply_to_user_id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [tweet_id, conversation_id, author_id, in_reply_to_user_id, created_at, geo, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)\n",
    "    \n",
    "def append_user_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for user in json_response['includes']['users']:\n",
    "\n",
    "        # 1. Author ID, Username and DisplayName\n",
    "        author_id = user['id']\n",
    "        username = user['username']\n",
    "        display_name = user['name']\n",
    "\n",
    "        # 2. Description\n",
    "        description = user['description']\n",
    "        \n",
    "        # 3. Verified\n",
    "        verified = user['verified']\n",
    "        \n",
    "        # 4. Time user created\n",
    "        created_at = dateutil.parser.parse(user['created_at'])\n",
    "\n",
    "        # 5. User metrics\n",
    "        followers_count = user['public_metrics']['followers_count']\n",
    "        following_count = user['public_metrics']['following_count']\n",
    "        tweet_count = user['public_metrics']['tweet_count']\n",
    "        listed_count = user['public_metrics']['listed_count']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, username, display_name, description, verified, created_at, followers_count, following_count, tweet_count, listed_count]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Users added from this response: \", counter) \n",
    "    \n",
    "def append_place_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for place in json_response['includes']['places']:\n",
    "\n",
    "        # Features\n",
    "        geo = place['id']\n",
    "        full_name = place['full_name']\n",
    "        place_type = place['place_type']\n",
    "        name = place['name']\n",
    "        country_code = place['country_code']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [geo, full_name, place_type, name, country_code]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Places added from this response: \", counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "priority_news = [\n",
    "    'CNN','MSNBC','NBCNews','FoxNews','BBCNews','itvnews','SkyNews','CTVNews','CBCNews','globalnews','inquirerdotnet','ABSCBNNews','gmanews','ndtv','timesofindia','TimesNow','republic',\n",
    "    '7NEWS','9NewsAUS','abcnews','News24','eNCA','SABCNews','rtenews','Independent_ie','thejournal_ie','MobilePunch','vanguardngrnews','PulseNigeria247','citizentvkenya','ntvkenya',\n",
    "    'NationAfrica','ntvuganda','nbstv','DailyMonitor','malaysiakini','staronline','NewshubNZ','nzherald'\n",
    "]\n",
    "\n",
    "#\n",
    "df = pd.read_csv('./../data/raw/news_tweets.csv')\n",
    "news_accounts = pd.read_csv('./../data/raw/covid_users.csv')\n",
    "tmp = df.author_id.value_counts().rename('amount').rename_axis('author_id').reset_index()\n",
    "news_accounts = news_accounts[news_accounts.author_id.isin(tmp.author_id)].drop_duplicates('author_id').merge(tmp).sort_values('amount', ascending=False)\n",
    "tweets_replies = pd.read_csv('./../data/raw/tweets_replies.csv')\n",
    "\n",
    "# Get selected conversations\n",
    "conversations = list(df[\n",
    "    (df.author_id.isin(news_accounts[news_accounts.username.isin(priority_news)].author_id.values)) &\n",
    "    ~(df.conversation_id.isin(tweets_replies.conversation_id.unique())) &\n",
    "    (df.reply_count>0)\n",
    "].conversation_id)\n",
    "random.shuffle(conversations)\n",
    "batches = list(split(conversations, int(len(conversations)/24)))\n",
    "\n",
    "# Inputs for tweets\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "start_date = '2020-01-01T00:00:00.000Z'\n",
    "end_date = '2021-11-14T23:59:59.000Z'\n",
    "max_results = 500\n",
    "\n",
    "# Loop Inputs\n",
    "total_tweets = 0\n",
    "count = 0 # Counting tweets per time period\n",
    "max_count = 10000000 # Max tweets per time period\n",
    "flag = True\n",
    "next_token = None\n",
    "n_requests = 0\n",
    "n_batches = 0\n",
    "valid = False\n",
    "errCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if flag is true\n",
    "start_time = time.time()\n",
    "\n",
    "for batch in batches:\n",
    "    \n",
    "    search = \"conversation_id:\" + \" OR conversation_id:\".join( [str(s) for s in batch]) + \" lang:en is:reply -is:retweet\"\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"Batch #: \", n_batches)\n",
    "    total_loop_tweets = 0\n",
    "    flag = True\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"Request #: \", n_requests+1, \" | Time cap: \", int(time.time() - start_time))\n",
    "        print(\"Token: \", next_token)\n",
    "        while not valid:\n",
    "            try:\n",
    "                url = create_url(search, start_date,end_date, max_results)\n",
    "                json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "                result_count = json_response['meta']['result_count']\n",
    "                n_requests += 1\n",
    "                valid = True; errCount = 0\n",
    "            except:\n",
    "                errCount += 1\n",
    "                time.sleep(2^errCount)\n",
    "                print(\"-------------------------------------- Request error #\", errCount)\n",
    "        valid = False\n",
    "\n",
    "        # Save the token to use for next call\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                append_tweet_to_csv(json_response, \"./../data/raw/tweets_replies.csv\")\n",
    "                append_user_to_csv(json_response, \"./../data/raw/users_replies.csv\")\n",
    "                if 'places' in json_response['includes'].keys():\n",
    "                    append_place_to_csv(json_response, \"./../data/raw/places_replies.csv\")\n",
    "                count += result_count\n",
    "                total_loop_tweets += result_count\n",
    "                print(\"Cumulative # of Tweets in this batch: \", total_loop_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(1)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                append_tweet_to_csv(json_response, \"./../data/raw/tweets_replies.csv\")\n",
    "                append_user_to_csv(json_response, \"./../data/raw/users_replies.csv\")\n",
    "                if 'places' in json_response['includes'].keys():\n",
    "                    append_place_to_csv(json_response, \"./../data/raw/places_replies.csv\")\n",
    "                count += result_count\n",
    "                total_loop_tweets += result_count\n",
    "                print(\"Cumulative # of Tweets in this batch: \", total_loop_tweets)\n",
    "                time.sleep(1)\n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(1)\n",
    "        # If reachs requests cap, stop it\n",
    "        t = time.time()-start_time\n",
    "        if n_requests==300:\n",
    "            if t<900:\n",
    "                time.sleep(900 - t)\n",
    "            start_time=time.time()\n",
    "            n_requests = 0\n",
    "    n_batches += 1\n",
    "    total_tweets += total_loop_tweets\n",
    "    print(\"Total number of batch results: \", total_loop_tweets)\n",
    "    print(\"Total # of Tweets added: \", total_tweets)\n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
